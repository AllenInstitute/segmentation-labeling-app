#!/bin/bash
#PBS -q braintv
#PBS -l nodes=1:ppn=2
#PBS -l mem=50g
#PBS -l walltime=00:10:00
#PBS -N launch_suite2p
#PBS -j oe
#PBS -o /allen/aibs/informatics/danielk/segmentation_results
#PBS -t 0-5

# activate a conda env with suite2p and ophys_segmentation installed
module load anaconda
conda_env=/allen/aibs/informatics/danielk/conda/djksuite2p
source activate ${conda_env}

# set env variables for talking to LIMS and mlflow
source /allen/aibs/informatics/danielk/segmentation_results/sourceme.sh

# manifest location for controlling this array job
manifest=/allen/aibs/informatics/danielk/segmentation_results/example_segmentation_manifest.json

# tell mlflow what to name this experiment and where to store the artifacts
experiment=labeling_jobs
artdir=/allen/aibs/informatics/danielk/artifacts

# based on ${PBS_ARRAYID} get some arguments for Suite2P
lims_id=$(python -c "import json; f = open('${manifest}', 'r'); j = json.load(f); f.close(); print(j['manifest'][${PBS_ARRAYID}]['experiment_id'])")
input_h5=$(python -c "import json; f = open('${manifest}', 'r'); j = json.load(f); f.close(); print(j['manifest'][${PBS_ARRAYID}]['input_video'])")
nbinned=$(python -c "import json; f = open('${manifest}', 'r'); j = json.load(f); f.close(); print(j['manifest'][${PBS_ARRAYID}]['nbinned'])")
echo lims_id: ${lims_id}
echo input_h5: ${input_h5}
echo nbinned: ${nbinned}
echo ""

# mlflow has some annoying collision on mkdir ./mlruns
# this avoids collisions
export TMPDIR=/scratch/capacity/${PBS_JOBID}
cd ${TMPDIR}

python -m segment.scripts.launch \
	--pipeline suite2p \
	--experiment_name ${experiment} \
	--artifact_location ${artdir}/${experiment} \
	--log_level DEBUG \
	--trained_classifier_path /allen/aibs/informatics/danielk/segmentation_test/trained_194.joblib \
	--conda_env_dir ${conda_env} \
	--save_path0 ${artdir}/${experiment} \
	--h5py ${input_h5} \
	--threshold_scaling 0.75 \
	--nbinned ${nbinned} \
	--save_h5 False \
	--keep_suite2p_results False \
        --fast_coo_output True \
        --write_to_postgres True
